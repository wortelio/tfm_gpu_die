MOBILENETV3 Classifier.
	One Head.
	Weighted for Precision.
	Dataset images divided by 255.


Datasets Length
	Train and Val: 128

Device: cuda
Optimizer:
	Learning Rate Freeze: 0.001
	Weight Decay Freeze: 0.001
	Learning Rate Fine Tuning: 1e-05
	Weight Decay Fine Tuning: 0.0001
Scheduler:
	Scheduler factor: 0.8
	Scheduler patience: 2
	Scheduler threshold: 0.001
	Scheduler min learning rate: 1e-06

Batch Size: 64
Num Workers: 8
Pin Memory: True
Epochs Freeze: 2
Epochs Fine Tuning: 3

IMG DIMS:
	Width: 224
	Height: 224

Trainable parameters = 18530
Total parameters = 945538


Torch Summary
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
PRETRAINED_MODEL                                        [1, 2]                    --
├─Sequential: 1-1                                       [1, 576, 1, 1]            --
│    └─Sequential: 2-1                                  [1, 576, 7, 7]            --
│    │    └─Conv2dNormActivation: 3-1                   [1, 16, 112, 112]         (464)
│    │    └─InvertedResidual: 3-2                       [1, 16, 56, 56]           (744)
│    │    └─InvertedResidual: 3-3                       [1, 24, 28, 28]           (3,864)
│    │    └─InvertedResidual: 3-4                       [1, 24, 28, 28]           (5,416)
│    │    └─InvertedResidual: 3-5                       [1, 40, 14, 14]           (13,736)
│    │    └─InvertedResidual: 3-6                       [1, 40, 14, 14]           (57,264)
│    │    └─InvertedResidual: 3-7                       [1, 40, 14, 14]           (57,264)
│    │    └─InvertedResidual: 3-8                       [1, 48, 14, 14]           (21,968)
│    │    └─InvertedResidual: 3-9                       [1, 48, 14, 14]           (29,800)
│    │    └─InvertedResidual: 3-10                      [1, 96, 7, 7]             (91,848)
│    │    └─InvertedResidual: 3-11                      [1, 96, 7, 7]             (294,096)
│    │    └─InvertedResidual: 3-12                      [1, 96, 7, 7]             (294,096)
│    │    └─Conv2dNormActivation: 3-13                  [1, 576, 7, 7]            (56,448)
│    └─AdaptiveAvgPool2d: 2-2                           [1, 576, 1, 1]            --
├─AdaptiveAvgPool2d: 1-2                                [1, 576, 1, 1]            --
├─Sequential: 1-3                                       [1, 2]                    --
│    └─Dropout: 2-3                                     [1, 576]                  --
│    └─Linear: 2-4                                      [1, 32]                   18,464
│    └─ReLU: 2-5                                        [1, 32]                   --
│    └─Dropout: 2-6                                     [1, 32]                   --
│    └─Linear: 2-7                                      [1, 2]                    66
=========================================================================================================
Total params: 945,538
Trainable params: 18,530
Non-trainable params: 927,008
Total mult-adds (M): 54.93
=========================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 22.63
Params size (MB): 3.78
Estimated Total Size (MB): 27.01
=========================================================================================================

Torch Model
PRETRAINED_MODEL(
  (base_model): Sequential(
    (0): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
      (1): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (2): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)
            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (3): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)
            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (4): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (5): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (6): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (7): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (8): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)
            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (9): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (10): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (11): InvertedResidual(
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): Hardswish()
          )
          (2): SqueezeExcitation(
            (avgpool): AdaptiveAvgPool2d(output_size=1)
            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))
            (activation): ReLU()
            (scale_activation): Hardsigmoid()
          )
          (3): Conv2dNormActivation(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
      (12): Conv2dNormActivation(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): Hardswish()
      )
    )
    (1): AdaptiveAvgPool2d(output_size=1)
  )
  (pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (linear): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=576, out_features=32, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.2, inplace=False)
    (4): Linear(in_features=32, out_features=2, bias=True)
  )
)

Loss Function: BCE
Smoke Precision Weight: 0.8
Starting script


***Start Training: 09:45:53


=== EPOCH 0/1 ===
Learning Rate = 0.001

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.5547   |0.7121   |0.1502   |0.2480   |
77.01      |38.55     |38.46     |    Fire   |0.6547   |0.4324   |0.1517   |0.2246   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6719   |0.9079   |0.3670   |0.5227   |
72.32      |36.94     |35.38     |    Fire   |0.7604   |0.6818   |0.3879   |0.4945   |

Saving model with new best validation loss: 72.3208
Saving model with best Mean F1: 0.5086

=== EPOCH 1/1 ===
Learning Rate = 0.001

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6859   |0.7745   |0.5048   |0.6112   |
65.16      |34.41     |30.75     |    Fire   |0.7750   |0.9589   |0.3318   |0.4930   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.7188   |0.7128   |0.7128   |0.7128   |
65.88      |35.02     |30.86     |    Fire   |0.8411   |0.7619   |0.6897   |0.7240   |

Saving model with new best validation loss: 65.8809
Saving model with best Mean F1: 0.7184
Saving last model

***Script finished: 09:46:03

Time elapsed: 0:00:10.068122

Trainable parameters = 945538
Total parameters = 945538

Fine Tuning


***Start Training: 09:46:03


=== EPOCH 2/4 ===
Learning Rate = 1e-05

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.7391   |0.8318   |0.5847   |0.6867   |
59.87      |32.29     |27.58     |    Fire   |0.8469   |0.9007   |0.6019   |0.7216   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.7240   |0.7253   |0.7021   |0.7135   |
64.34      |34.53     |29.81     |    Fire   |0.8359   |0.7732   |0.6466   |0.7042   |

Saving model with new best validation loss: 64.3404
Saving model with best Mean F1: 0.7089

=== EPOCH 3/4 ===
Learning Rate = 1e-05

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.7125   |0.7792   |0.5751   |0.6618   |
59.43      |32.94     |26.50     |    Fire   |0.8625   |0.9301   |0.6303   |0.7514   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.7214   |0.7314   |0.6809   |0.7052   |
63.01      |34.07     |28.94     |    Fire   |0.8464   |0.7938   |0.6638   |0.7230   |

Saving model with new best validation loss: 63.0136
Saving model with best Mean F1: 0.7141

=== EPOCH 4/4 ===
Learning Rate = 1e-05

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.7266   |0.8080   |0.5783   |0.6741   |
58.17      |32.12     |26.05     |    Fire   |0.8562   |0.8993   |0.6351   |0.7444   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.7266   |0.7399   |0.6809   |0.7091   |
61.72      |33.63     |28.09     |    Fire   |0.8542   |0.8191   |0.6638   |0.7333   |

Saving model with new best validation loss: 61.7240
Saving model with best Mean F1: 0.7212
Saving last model

***Script finished: 09:46:19

Time elapsed: 0:00:15.765291
