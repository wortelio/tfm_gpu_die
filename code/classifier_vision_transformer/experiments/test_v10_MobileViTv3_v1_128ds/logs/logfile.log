MobileViTv3_v1 Classifier.
	One Head.
	Weighted for Precision.
	Dataset images divided by 255.


Datasets Length
	Train and Val: 128
	Add Clouds: False

Load Model: False

Device: cuda
Optimizer:
	Learning Rate: 0.001
	Weight Decay: 0.001
Scheduler:
	Scheduler factor: 0.8
	Scheduler patience: 2
	Scheduler threshold: 0.001
	Scheduler min learning rate: 1e-06

Batch Size: 64
Num Workers: 8
Pin Memory: True
Epochs: 5

IMG DIMS:
	Width: 224
	Height: 224

Brevitas Config:
	Fixed Point: True
	Weights Bit Width: 4
	Big Layers Weights Bit Width: 4
	Bias Bit Width: 4
	Activations Bit Width: 4
Train Dataset Length: 640
Test Dataset Length: 384

Trainable parameters = 737474
Total parameters = 737474


Torch Summary
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
MobileViTv3_v1                                     [1, 2]                    --
├─Sequential: 1-1                                  [1, 16, 111, 111]         --
│    └─Conv2d: 2-1                                 [1, 16, 111, 111]         432
│    └─BatchNorm2d: 2-2                            [1, 16, 111, 111]         32
│    └─SiLU: 2-3                                   [1, 16, 111, 111]         --
├─Sequential: 1-2                                  [1, 16, 111, 111]         --
│    └─InvertedResidual: 2-4                       [1, 16, 111, 111]         --
│    │    └─Sequential: 3-1                        [1, 16, 111, 111]         1,472
├─Sequential: 1-3                                  [1, 24, 56, 56]           --
│    └─InvertedResidual: 2-5                       [1, 24, 56, 56]           --
│    │    └─Sequential: 3-2                        [1, 24, 56, 56]           1,744
│    └─InvertedResidual: 2-6                       [1, 24, 56, 56]           --
│    │    └─Sequential: 3-3                        [1, 24, 56, 56]           2,976
│    └─InvertedResidual: 2-7                       [1, 24, 56, 56]           --
│    │    └─Sequential: 3-4                        [1, 24, 56, 56]           2,976
├─Sequential: 1-4                                  [1, 64, 28, 28]           --
│    └─InvertedResidual: 2-8                       [1, 64, 28, 28]           --
│    │    └─Sequential: 3-5                        [1, 64, 28, 28]           4,976
│    └─MobileViTBlockV3_v1: 2-9                    [1, 64, 28, 28]           --
│    │    └─Sequential: 3-6                        [1, 64, 28, 28]           4,800
│    │    └─Sequential: 3-7                        [4, 196, 64]              67,072
│    │    └─Sequential: 3-8                        [1, 64, 28, 28]           4,224
│    │    └─Sequential: 3-9                        [1, 64, 28, 28]           8,320
├─Sequential: 1-5                                  [1, 80, 14, 14]           --
│    └─InvertedResidual: 2-10                      [1, 80, 14, 14]           --
│    │    └─Sequential: 3-10                       [1, 80, 14, 14]           20,256
│    └─MobileViTBlockV3_v1: 2-11                   [1, 80, 14, 14]           --
│    │    └─Sequential: 3-11                       [1, 80, 14, 14]           7,280
│    │    └─Sequential: 3-12                       [4, 49, 80]               208,480
│    │    └─Sequential: 3-13                       [1, 80, 14, 14]           6,560
│    │    └─Sequential: 3-14                       [1, 80, 14, 14]           12,960
├─Sequential: 1-6                                  [1, 128, 7, 7]            --
│    └─InvertedResidual: 2-12                      [1, 128, 7, 7]            --
│    │    └─Sequential: 3-15                       [1, 128, 7, 7]            35,616
│    └─MobileViTBlockV3_v1: 2-13                   [1, 128, 7, 7]            --
│    │    └─Sequential: 3-16                       [1, 96, 7, 7]             13,696
│    │    └─Sequential: 3-17                       [4, 16, 96]               224,544
│    │    └─Sequential: 3-18                       [1, 128, 7, 7]            12,544
│    │    └─Sequential: 3-19                       [1, 128, 7, 7]            28,928
├─Sequential: 1-7                                  [1, 512, 7, 7]            --
│    └─Conv2d: 2-14                                [1, 512, 7, 7]            65,536
│    └─BatchNorm2d: 2-15                           [1, 512, 7, 7]            1,024
│    └─SiLU: 2-16                                  [1, 512, 7, 7]            --
├─Linear: 1-8                                      [1, 2]                    1,026
====================================================================================================
Total params: 737,474
Trainable params: 737,474
Non-trainable params: 0
Total mult-adds (M): 93.21
====================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 64.96
Params size (MB): 2.95
Estimated Total Size (MB): 68.51
====================================================================================================

Torch Model
MobileViTv3_v1(
  (conv_0): Sequential(
    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), bias=False)
    (BatchNorm2d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (Activation): SiLU()
  )
  (layer_1): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): Sequential(
          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_3x3): Sequential(
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (red_1x1): Sequential(
          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (layer_2): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): Sequential(
          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_3x3): Sequential(
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
          (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (red_1x1): Sequential(
          (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (1): InvertedResidual(
      (block): Sequential(
        (exp_1x1): Sequential(
          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_3x3): Sequential(
          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
          (BatchNorm2d): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (red_1x1): Sequential(
          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (2): InvertedResidual(
      (block): Sequential(
        (exp_1x1): Sequential(
          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_3x3): Sequential(
          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
          (BatchNorm2d): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (red_1x1): Sequential(
          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (layer_3): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): Sequential(
          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_3x3): Sequential(
          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
          (BatchNorm2d): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (red_1x1): Sequential(
          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (1): MobileViTBlockV3_v1(
      (local_rep): Sequential(
        (conv_3x3): Sequential(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_1x1): Sequential(
          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (global_rep): Sequential(
        (TransformerEncoder_0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=64, out_features=192, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=64, out_features=128, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=128, out_features=64, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (TransformerEncoder_1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=64, out_features=192, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=64, out_features=128, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=128, out_features=64, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (LayerNorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): Sequential(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (Activation): SiLU()
      )
      (fusion): Sequential(
        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (Activation): SiLU()
      )
    )
  )
  (layer_4): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): Sequential(
          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_3x3): Sequential(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
          (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (red_1x1): Sequential(
          (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (1): MobileViTBlockV3_v1(
      (local_rep): Sequential(
        (conv_3x3): Sequential(
          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
          (BatchNorm2d): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_1x1): Sequential(
          (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (global_rep): Sequential(
        (TransformerEncoder_0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=80, out_features=240, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=80, out_features=80, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=80, out_features=160, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (TransformerEncoder_1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=80, out_features=240, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=80, out_features=80, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=80, out_features=160, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (TransformerEncoder_2): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=80, out_features=240, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=80, out_features=80, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=80, out_features=160, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (TransformerEncoder_3): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=80, out_features=240, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=80, out_features=80, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=80, out_features=160, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=160, out_features=80, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (LayerNorm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): Sequential(
        (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (BatchNorm2d): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (Activation): SiLU()
      )
      (fusion): Sequential(
        (conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (BatchNorm2d): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (Activation): SiLU()
      )
    )
  )
  (layer_5): Sequential(
    (0): InvertedResidual(
      (block): Sequential(
        (exp_1x1): Sequential(
          (conv): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_3x3): Sequential(
          (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
          (BatchNorm2d): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (red_1x1): Sequential(
          (conv): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (1): MobileViTBlockV3_v1(
      (local_rep): Sequential(
        (conv_3x3): Sequential(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (Activation): SiLU()
        )
        (conv_1x1): Sequential(
          (conv): Conv2d(128, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (global_rep): Sequential(
        (TransformerEncoder_0): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=96, out_features=288, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=96, out_features=192, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=192, out_features=96, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (TransformerEncoder_1): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=96, out_features=288, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=96, out_features=192, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=192, out_features=96, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (TransformerEncoder_2): TransformerEncoder(
          (pre_norm_mha): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): Attention(
              (qkv_proj): Linear(in_features=96, out_features=288, bias=True)
              (softmax): Softmax(dim=-1)
              (attn_dropout): Dropout(p=0, inplace=False)
              (out_proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (2): Dropout(p=0, inplace=False)
          )
          (pre_norm_ffn): Sequential(
            (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=96, out_features=192, bias=True)
            (2): SiLU()
            (3): Dropout(p=0, inplace=False)
            (4): Linear(in_features=192, out_features=96, bias=True)
            (5): Dropout(p=0, inplace=False)
          )
        )
        (LayerNorm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (conv_proj): Sequential(
        (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (Activation): SiLU()
      )
      (fusion): Sequential(
        (conv): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (Activation): SiLU()
      )
    )
  )
  (conv_1x1_exp): Sequential(
    (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (Activation): SiLU()
  )
  (out): Linear(in_features=512, out_features=2, bias=True)
)

Loss Function: BCE
Smoke Precision Weight: 0.8
Starting script


***Start Training: 00:03:55


=== EPOCH 0/4 ===
Learning Rate = 0.001

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6000   |0.6052   |0.5240   |0.5616   |
72.96      |37.72     |35.25     |    Fire   |0.7359   |0.6167   |0.5261   |0.5678   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.5104   |0.0000   |0.0000   |0.0000   |
78.84      |39.58     |39.26     |    Fire   |0.6979   |0.0000   |0.0000   |0.0000   |

Saving model with new best validation loss: 78.8352

=== EPOCH 1/4 ===
Learning Rate = 0.001

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6156   |0.6959   |0.3802   |0.4917   |
64.91      |36.95     |27.97     |    Fire   |0.8047   |0.7287   |0.6493   |0.6867   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.4896   |0.4896   |1.0000   |0.6573   |
81.36      |43.09     |38.27     |    Fire   |0.6979   |0.0000   |0.0000   |0.0000   |

Saving model with best Mean F1: 0.3287

=== EPOCH 2/4 ===
Learning Rate = 0.001

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6594   |0.6987   |0.5335   |0.6051   |
61.58      |35.58     |26.00     |    Fire   |0.8094   |0.7171   |0.6967   |0.7067   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.4922   |0.4909   |1.0000   |0.6585   |
81.68      |46.33     |35.35     |    Fire   |0.8021   |0.7500   |0.5172   |0.6122   |

Saving model with best Mean F1: 0.6354

=== EPOCH 3/4 ===
Learning Rate = 0.001

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6484   |0.6897   |0.5112   |0.5872   |
60.82      |34.82     |26.00     |    Fire   |0.8219   |0.7594   |0.6730   |0.7136   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.5365   |0.5150   |0.9149   |0.6590   |
76.26      |43.83     |32.43     |    Fire   |0.7422   |0.5486   |0.8276   |0.6598   |

Saving model with new best validation loss: 76.2580
Saving model with best Mean F1: 0.6594

=== EPOCH 4/4 ===
Learning Rate = 0.001

TRAIN Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6516   |0.6744   |0.5559   |0.6095   |
59.36      |34.29     |25.07     |    Fire   |0.8203   |0.7330   |0.7156   |0.7242   |

VAL Stats
Total Loss |Smoke Loss|Fire Loss |    _______|Accuracy |Precision|Recall   |F1       |
-----------|----------|----------|    Smoke  |0.6224   |0.6937   |0.4096   |0.5151   |
67.20      |36.10     |31.10     |    Fire   |0.7604   |0.6395   |0.4741   |0.5446   |

Saving model with new best validation loss: 67.1973
Saving last model

***Script finished: 00:04:22

Time elapsed: 0:00:26.915175

Testing with FULL TEST LOADER
{'Accuracy': [0.5364583134651184, 0.7421875], 'Precision': [0.514970064163208, 0.5485714077949524], 'Recall': [0.914893627166748, 0.8275862336158752], 'F1': [0.6590038537979126, 0.6597937941551208]}

Testing with DFire MINI TRAIN after LOADING F1 Best Mean CHECKPOINT
{'Accuracy': [0.699999988079071, 0.800000011920929], 'Precision': [0.7528089880943298, 0.7142857313156128], 'Recall': [0.8933333158493042, 0.7142857313156128], 'F1': [0.8170731663703918, 0.7142857313156128]}

Testing with DFire MINI TEST after LOADING F1 Best Mean CHECKPOINT
{'Accuracy': [0.6666666865348816, 0.8333333134651184], 'Precision': [0.7142857313156128, 0.7142857313156128], 'Recall': [0.9090909361839294, 0.9090909361839294], 'F1': [0.800000011920929, 0.800000011920929]}
Datasets Length
Train Dataset Length: 640
Test Dataset Length: 384

Datasets Length
Train Dataset Length: 640
Test Dataset Length: 384

********* Datasets Length *********
Train Dataset Length: 640
Test Dataset Length: 384
