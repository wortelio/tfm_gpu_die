{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284a093f-4bbf-4132-9e9d-7283cb06f19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import config\n",
    "import modules.dataloaders as data_loaders\n",
    "#import modules.utils as utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "\n",
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf727a7-427b-4254-a8c8-0628d75b267b",
   "metadata": {},
   "source": [
    "# Save Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd33516-04da-4a81-8b0a-a81432680f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_folder = './predicted_images/onnx_classify_thres_&_detect/'\n",
    "save_only_detection = save_folder + 'only_detection'\n",
    "save_classify_05_detect_02 = save_folder + 'classify_050_detect_2e-1'\n",
    "save_classify_03_detect_02 = save_folder + 'classify_030_detect_2e-1'\n",
    "# save_classify_detect_01 = save_folder + 'classify_detect_1e-1'\n",
    "# save_classify_detect_001 = save_folder + 'classify_detect_1e-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040f29a-9db4-4419-a6f6-4b1bcb9e3bdf",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Change DS_LEN in config, so list is shuffled first and VAL Datasets are more diverse. To draw 128 pictures, choose DS_LEN = 168, so it drops some overlapped and mora than x objects, but you still have 128 pictures.\n",
    "\n",
    "RS Dataset length is about 342 pictures with MAX_OBJ = 5, so 128 is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868a232-7f68-4d89-9338-4967e05b41e8",
   "metadata": {},
   "source": [
    "### Random Seed\n",
    "\n",
    "Initialize it to shuffle list inside datasets always in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d44894-4336-45ee-8e57-90223af2c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007c4231-32bd-4517-b977-e173132ce533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST DFire dataset\n",
      "DFire Removed wrong images: 0\n",
      "DFire Removed due to overlapping: 20\n",
      "DFire Removed due to more than 10: 0\n",
      "\n",
      "Test DFire dataset len: 180\n",
      "\n",
      "TEST FASDD UAV dataset\n",
      "FASDD Removed wrong images: 0\n",
      "FASDD Removed due to overlapping: 27\n",
      "FASDD Removed due to more than 10: 12\n",
      "\n",
      "Test FASDD UAV dataset len: 161\n",
      "\n",
      "TEST FASDD CV dataset\n",
      "FASDD Removed wrong images: 0\n",
      "FASDD Removed due to overlapping: 6\n",
      "FASDD Removed due to more than 10: 2\n",
      "\n",
      "Test FASDD CV dataset len: 192\n",
      "\n",
      "Concatenate Test DFire and FASDD UAV datasets\n",
      "Test dataset len: 341\n",
      "Concatenate with FASDD CV dataset\n",
      "Test dataset len: 533\n"
     ]
    }
   ],
   "source": [
    "val_loader = data_loaders.get_val_loader(\n",
    "    dfire_len = 200,\n",
    "    fasdd_uav_len = 200,\n",
    "    fasdd_cv_len = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b68c8-740b-4f15-aa8e-22fac9b08bf0",
   "metadata": {},
   "source": [
    "# Check ONNX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04853bb2-16a6-4bb1-80ee-8a0a06c8bd37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = onnx.load('./onnx_models/medium_fassd__conv341_big__epoch=93.onnx')\n",
    "onnx.checker.check_model(classifier)\n",
    "\n",
    "detector = onnx.load('./onnx_models/w8a8b8__bed_detector___aimet__fixed_point__qcdq__CPU.onnx')\n",
    "onnx.checker.check_model(detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690309a-2b41-4bb7-9ebd-17a934ca6d9a",
   "metadata": {},
   "source": [
    "# Functions to Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e2476b-9e2a-45fe-bc19-592b99d8d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d775ea36-106b-40d3-bfea-56c1ee44ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(\n",
    "    boxes_preds, boxes_labels, \n",
    "    box_format=\"midpoint\",\n",
    "    epsilon=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union for bounding boxes.\n",
    "    \n",
    "    :param boxes_preds (tensor): Bounding box predictions of shape (BATCH_SIZE, 4)\n",
    "    :param boxes_labels (tensor): Ground truth bounding box of shape (BATCH_SIZE, 4)\n",
    "    :param box_format (str): midpoint/corners, if boxes (x,y,w,h) format or (x1,y1,x2,y2) format\n",
    "    :param epsilon: Small value to prevent division by zero.\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == 'midpoint':\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == 'corners':\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4] \n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    union = (box1_area + box2_area - intersection + epsilon)\n",
    "\n",
    "    iou = intersection / union\n",
    "    #print(f'IOU is numpy: {iou.numpy()}')\n",
    "\n",
    "    return iou\n",
    "\n",
    "def non_max_supression(bboxes, \n",
    "                       iou_threshold=config.IOU_THRESHOLD, \n",
    "                       score_threshold=config.SCORE_THRESHOLD, \n",
    "                       box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [x1, y1, x2, y2, confidence, class_id] MY FORMAT VERSION       \n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        score_threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[4] > score_threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[4], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[5] != chosen_box[5]\n",
    "            or iou(\n",
    "                torch.tensor(chosen_box[:4]),\n",
    "                torch.tensor(box[:4]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "\n",
    "# ______________________________________________________________ #\n",
    "# ____________________      Pred Boxes      ____________________ #\n",
    "# ______________________________________________________________ #\n",
    "def get_best_box_and_class(out):\n",
    "    \n",
    "    conf_1 = out[..., 4:5]\n",
    "    conf_2 = out[..., 9:10]\n",
    "    confs = torch.cat((conf_1, conf_2), dim=-1)\n",
    "    _, idx = torch.max(confs, keepdim=True, dim=-1)\n",
    "    \n",
    "    best_boxes = idx*out[..., 5:10] + (1-idx)*out[..., 0:5]\n",
    "    \n",
    "    _, class_idx = torch.max(out[..., 10:12], keepdim=True, dim=-1)\n",
    "    \n",
    "    best_out = torch.cat((best_boxes, class_idx), dim=-1)\n",
    "\n",
    "    return best_out\n",
    "\n",
    "def get_bboxes_from_model_out(model_out, \n",
    "                              iou_threshold=config.IOU_THRESHOLD, \n",
    "                              score_threshold=config.SCORE_THRESHOLD):\n",
    "\n",
    "    model_out = get_best_box_and_class(model_out.detach().to('cpu'))\n",
    "    \n",
    "    c2b_mtx = np.zeros((config.S, config.S, 2))\n",
    "    for j in range(config.S):\n",
    "        for i in range(config.S):\n",
    "            c2b_mtx[i, j, 0] = j\n",
    "            c2b_mtx[i, j, 1] = i\n",
    "\n",
    "    model_out = model_out.numpy()\n",
    "    out_xy = model_out[..., :2]\n",
    "    out_rest = model_out[..., 2:]\n",
    "\n",
    "    c2b_xy = (c2b_mtx+out_xy)/config.S\n",
    "    out = np.concatenate((c2b_xy, out_rest), axis=-1)\n",
    "    #print(f'Concat out\\n {out}')\n",
    "\n",
    "    bboxes_flat = np.reshape(out, (config.S*config.S, 5+1)) # Replace 5+C by 5+1, as we filtered best class before (get_best_box_and_class)\n",
    "    bboxes_list = [bbox for bbox in bboxes_flat.tolist()]\n",
    "\n",
    "    nms_pred_bboxes = non_max_supression(\n",
    "        bboxes_list,\n",
    "        iou_threshold=iou_threshold, \n",
    "        score_threshold=score_threshold, \n",
    "        box_format=\"midpoint\")\n",
    "\n",
    "    return nms_pred_bboxes\n",
    "\n",
    "\n",
    "# ______________________________________________________________ #\n",
    "# ____________________      True Boxes      ____________________ #\n",
    "# ______________________________________________________________ #\n",
    "def get_bboxes_from_label_mtx(label_mtx):\n",
    "    '''\n",
    "    Receives a label_mtx, as yielded by dataset or dataloader and returns a list of bounding boxes.\n",
    "    \n",
    "    Arguments:\n",
    "        - label_mtx\n",
    "    \n",
    "    Returns:\n",
    "        - bboxes_list: list with all cells containing score = 1\n",
    "            [xcell, ycell, w, h, score, smoke, fire] -> [x, y, w, h, 1, smoke, fire]\n",
    "    '''\n",
    "\n",
    "    c2b_mtx = np.zeros((config.S, config.S, 2))\n",
    "    for j in range(config.S):\n",
    "        for i in range(config.S):\n",
    "            c2b_mtx[i, j, 0] = j\n",
    "            c2b_mtx[i, j, 1] = i\n",
    "\n",
    "    label_mtx = label_mtx.numpy()\n",
    "    label_xy = label_mtx[..., :2]\n",
    "    label_rest = label_mtx[..., 2:]\n",
    "\n",
    "    c2b_xy = (c2b_mtx+label_xy)/config.S\n",
    "    out = np.concatenate((c2b_xy, label_rest), axis=-1)\n",
    "    #print(f'Concat out\\n {out}')\n",
    "\n",
    "    bboxes_list = np.reshape(out, (config.S*config.S, 5+config.C))\n",
    "\n",
    "    bboxes_list = [bbox for bbox in bboxes_list.tolist() if bbox[4]==1]\n",
    "\n",
    "    return bboxes_list\n",
    "\n",
    "\n",
    "# ______________________________________________________________ #\n",
    "# ____________________        Plots         ____________________ #\n",
    "# ____________________ True & Pred Boxes    ____________________ #\n",
    "# ______________________________________________________________ #\n",
    "def plot_grid(img):\n",
    "    '''\n",
    "    Plot grid on top of the picture\n",
    "    '''\n",
    "      \n",
    "    cell_size = int(config.IMG_W / config.S)\n",
    "    \n",
    "    # Draw horizontal lines\n",
    "    for i in range(1, config.S):\n",
    "        cv2.line(img, (0, cell_size*i), (config.IMG_W-1, cell_size*i), config.GRID_COLOR, 1)\n",
    "    # Draw vertical lines\n",
    "    for j in range(1, config.S):\n",
    "        cv2.line(img, (cell_size*j, 0), (cell_size*j, config.IMG_H-1), config.GRID_COLOR, 1)\n",
    "        \n",
    "    return img\n",
    "\n",
    "    \n",
    "def plot_dataset_img(img, label_mtx, grid=False):\n",
    "    '''\n",
    "    It draws the bounding boxes over the image.\n",
    "\n",
    "    Arguments:\n",
    "        - ori_img: original image with no modification or letterbox\n",
    "        - label_mtx: [xcell, ycell, w, h, score=1, smoke, fire], tensor (7, 7, 12)\n",
    "        - grid: plot grid over the image\n",
    "\n",
    "    Returns:\n",
    "        - pic: picture with bounding boxes on top of original picture\n",
    "    '''\n",
    "\n",
    "    # NEVER remove copy() or use np.ascontiguousarray()\n",
    "    # Convert pytorch tensor to numpy\n",
    "    img = img.permute(1, 2, 0) * 256\n",
    "    img = img.numpy().astype(np.uint8).copy()   \n",
    "       \n",
    "    if grid == True:\n",
    "        img = plot_grid(img)\n",
    "    \n",
    "    bboxes = get_bboxes_from_label_mtx(label_mtx)\n",
    "\n",
    "    for i,(xc, yc, w, h, score, smoke, fire) in enumerate(bboxes):\n",
    "        xmin, ymin, xmax, ymax = xc - w/2, yc - h/2, xc + w/2, yc + h/2\n",
    "        box = np.array([xmin, ymin, xmax, ymax]).astype(np.float32)\n",
    "        box[0] = box[0]*config.IMG_W\n",
    "        np.clip(box[0], 0, None)\n",
    "        box[1] = box[1]*config.IMG_H\n",
    "        np.clip(box[1], 0, None)\n",
    "        box[2] = box[2]*config.IMG_W - 1 # avoid out of limits due to rounding\n",
    "        box[3] = box[3]*config.IMG_H - 1 # avoid out of limits due to rounding\n",
    "        box = box.round().astype(np.int32).tolist()\n",
    "        #print(f'Box after conversion\\n{box}')\n",
    "        if smoke == 1:\n",
    "            class_id = 0\n",
    "        elif fire == 1:\n",
    "            class_id = 1\n",
    "        else:\n",
    "            print(\"Wrong Class ID\")\n",
    "        name = config.CLASSES[class_id]\n",
    "        color = config.BBOX_COLORS[name]\n",
    "        cv2.rectangle(img, box[:2], box[2:], color, 1) \n",
    "        if box[1] < 30:\n",
    "            if class_id == 0:\n",
    "                cv2.rectangle(img, [box[0], box[1]+15], [box[0]+55, box[1]], color, -1) \n",
    "            else:\n",
    "                cv2.rectangle(img, [box[0], box[1]+15], [box[0]+25, box[1]], color, -1) \n",
    "            cv2.putText(img,name,(box[0], box[1] + 12),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 0],\n",
    "                        thickness=1)  # 0.5 -> font size\n",
    "        else:\n",
    "            if class_id == 0:\n",
    "                cv2.rectangle(img, [box[0], box[1]-20], [box[0]+55, box[1]], color, -1) \n",
    "            else:\n",
    "                cv2.rectangle(img, [box[0], box[1]-20], [box[0]+25, box[1]], color, -1) \n",
    "            cv2.putText(img,name,(box[0], box[1] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 0],\n",
    "                        thickness=1)  # 0.5 -> font size\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_predicted_img(img, model_out, score_thres=None, grid=False):\n",
    "    '''\n",
    "    It draws the bounding boxes over the image.\n",
    "\n",
    "    Arguments:\n",
    "        - ori_img: original image with no modification or letterbox\n",
    "        - model_out: [xcell, ycell, w, h, score, smoke, fire], tensor (7, 7, 12)\n",
    "        - grid: plot grid over the image\n",
    "\n",
    "    Returns:\n",
    "        - pic: picture with bounding boxes on top of original picture\n",
    "    '''\n",
    "\n",
    "    # NEVER remove copy() or use np.ascontiguousarray()\n",
    "    # Convert pytorch tensor to numpy\n",
    "    img = img.permute(1, 2, 0) * 256\n",
    "    img = img.numpy().astype(np.uint8).copy()   \n",
    "       \n",
    "    if grid == True:\n",
    "        img = plot_grid(img)\n",
    "    \n",
    "    if score_thres is not None:\n",
    "        bboxes = get_bboxes_from_model_out(\n",
    "            model_out,\n",
    "            score_threshold=score_thres)\n",
    "    else:\n",
    "        bboxes = get_bboxes_from_model_out(model_out)\n",
    "\n",
    "    for xc, yc, w, h, score, class_id in bboxes:\n",
    "        xmin, ymin, xmax, ymax = xc - w/2, yc - h/2, xc + w/2, yc + h/2\n",
    "        box = np.array([xmin, ymin, xmax, ymax]).astype(np.float32)\n",
    "        box[0] = box[0]*config.IMG_W\n",
    "        np.clip(box[0], 0, None)\n",
    "        box[1] = box[1]*config.IMG_H\n",
    "        np.clip(box[1], 0, None)\n",
    "        box[2] = box[2]*config.IMG_W - 1 # avoid out of limits due to rounding\n",
    "        box[3] = box[3]*config.IMG_H - 1 # avoid out of limits due to rounding\n",
    "        box = box.round().astype(np.int32).tolist()\n",
    "        \n",
    "        class_id = int(class_id)\n",
    "        name = config.CLASSES[class_id]\n",
    "        color = config.BBOX_COLORS[name]\n",
    "        name += str(f' {score:.3f}')\n",
    "        \n",
    "        cv2.rectangle(img, box[:2], box[2:], color, 1) \n",
    "        if box[1] < 30:\n",
    "            if class_id == 0:\n",
    "                cv2.rectangle(img, [box[0], box[1]+15], [box[0]+105, box[1]], color, -1) \n",
    "            else:\n",
    "                cv2.rectangle(img, [box[0], box[1]+15], [box[0]+80, box[1]], color, -1) \n",
    "            cv2.putText(img,name,(box[0], box[1] + 12),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 0],\n",
    "                        thickness=1)  # 0.5 -> font size\n",
    "        else:\n",
    "            if class_id == 0:\n",
    "                cv2.rectangle(img, [box[0], box[1]-20], [box[0]+105, box[1]], color, -1) \n",
    "            else:\n",
    "                cv2.rectangle(img, [box[0], box[1]-20], [box[0]+80, box[1]], color, -1) \n",
    "            cv2.putText(img,name,(box[0], box[1] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 0, 0],\n",
    "                        thickness=1)  # 0.5 -> font size\n",
    "\n",
    "    return img\n",
    "\n",
    "# ______________________________________________________________ #\n",
    "# _________________ ONNX Prediction Function  __________________ #\n",
    "# ______________________________________________________________ #\n",
    "def onnx_predict(\n",
    "    img,\n",
    "    classify_session,\n",
    "    detect_session,\n",
    "    classification_thres\n",
    "):\n",
    "    \n",
    "    if classify_session is not None:\n",
    "        classify_inputs = {classify_session.get_inputs()[0].name: to_numpy(img)}\n",
    "        classification_out = classify_session.run(None, classify_inputs)\n",
    "        # print(f'Smoke pred: {classification_out[0][0][0]}')\n",
    "        # print(f'Fire pred: {classification_out[0][0][1]}')\n",
    "\n",
    "        # Use Detector if Classifier predicts fire or smoke\n",
    "        if ( classification_out[0][0][0] >= classification_thres\n",
    "            or \n",
    "            classification_out[0][0][1] >= classification_thres ):\n",
    "            detect_inputs = {detect_session.get_inputs()[0].name: to_numpy(img)}\n",
    "            out = detect_session.run(None, detect_inputs)\n",
    "            out = torch.tensor(np.array(out[0]))\n",
    "            out = out.permute(0, 2, 3, 1)\n",
    "        else:\n",
    "            out = torch.zeros(1,7,7,12)  \n",
    "    \n",
    "    else:\n",
    "        detect_inputs = {detect_session.get_inputs()[0].name: to_numpy(img)}\n",
    "        out = detect_session.run(None, detect_inputs)\n",
    "        out = torch.tensor(np.array(out[0]))\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        \n",
    "    # Remove batch dim, although it makes no difference for get_bboxes_from_model_out \n",
    "    out = out[0]   \n",
    "    \n",
    "    return out\n",
    "        \n",
    "# ______________________________________________________________ #\n",
    "# ____________________         Plot         ____________________ #\n",
    "# ____________________      N IMAGES        ____________________ #\n",
    "# _________________ [ori1, pred1, ori2, pred2] _________________ #\n",
    "# ______________________________________________________________ #\n",
    "def plot_n_images_onnx(\n",
    "    loader, \n",
    "    classification_model,\n",
    "    detection_model,\n",
    "    classification_thres,\n",
    "    score_thres,\n",
    "    n_imgs, \n",
    "    save_name):\n",
    "    '''\n",
    "    Plots 4 pictures in each row: [ori1, pred1, ori2, pred2]\n",
    "    '''\n",
    "    \n",
    "    if classification_model is not None:\n",
    "        classify_session = onnxruntime.InferenceSession(classification_model, providers=[\"CPUExecutionProvider\"])\n",
    "    else:\n",
    "        classify_session = None\n",
    "    detect_session = onnxruntime.InferenceSession(detection_model, providers=[\"CPUExecutionProvider\"])\n",
    "    \n",
    "    n_imgs = int(n_imgs / 2) * 2 # Make n_imgs an even number\n",
    "    rows = int(n_imgs/2)\n",
    "    cols = 4\n",
    "    plot_img_height = n_imgs\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(10, n_imgs)) # 32 pics -> (10, 40) / 64 \n",
    "    \n",
    "    img_idx = 0\n",
    "    \n",
    "    for (img, label) in loader:\n",
    "        \n",
    "        half_batch = int(config.BATCH_SIZE/2)\n",
    "        for i in range(half_batch): # index for Half Batch_Size\n",
    "            \n",
    "            #print(f'Batch Index: {i}')\n",
    "            ax_idx = int(img_idx/4)\n",
    "            #print(f'AX idx: {ax_idx}')\n",
    "            #print(f'Img idx before update: {img_idx}')\n",
    "    \n",
    "            # Left Half: Original, Predicted\n",
    "            plt.subplot(rows, cols, img_idx+1)\n",
    "            if img_idx == 0:\n",
    "                ax[ax_idx][0].set_title(\"Original\") # Set title for colum 0\n",
    "\n",
    "            ori_pic_1 = plot_dataset_img(img[2*i], label[2*i], grid=False)\n",
    "            ax[ax_idx][0].imshow(ori_pic_1)\n",
    "            ax[ax_idx][0].set_axis_off()\n",
    "\n",
    "            plt.subplot(rows, cols, img_idx+2)\n",
    "            if img_idx == 0:\n",
    "                ax[ax_idx][1].set_title(\"Predicted\") # Set title for colum 1\n",
    "\n",
    "            img_to_model_1 = img[2*i].unsqueeze(dim=0) #.to(config.DEVICE)\n",
    "            # pred_out_1 = model(img_to_model_1)\n",
    "            # pred_out_1 = pred_out_1.permute(0, 2, 3, 1)\n",
    "            \n",
    "            pred_out_1 = onnx_predict(\n",
    "                img_to_model_1,\n",
    "                classify_session,\n",
    "                detect_session,\n",
    "                classification_thres=classification_thres\n",
    "            )\n",
    "                \n",
    "            pred_pic_1 = plot_predicted_img(\n",
    "                img[2*i], \n",
    "                pred_out_1, \n",
    "                score_thres=score_thres,\n",
    "                grid=False)\n",
    "            \n",
    "            ax[ax_idx][1].imshow(pred_pic_1)\n",
    "            ax[ax_idx][1].set_axis_off()\n",
    "\n",
    "            # Right Half: Original, Predicted\n",
    "            plt.subplot(rows, cols, img_idx+3)\n",
    "            if img_idx == 0:\n",
    "                ax[ax_idx][2].set_title(\"Original\") # Set title for colum 2\n",
    "\n",
    "            ori_pic_2 = plot_dataset_img(img[2*i+1], label[2*i+1], grid=False)\n",
    "            ax[ax_idx][2].imshow(ori_pic_2)\n",
    "            ax[ax_idx][2].set_axis_off()\n",
    "\n",
    "            plt.subplot(rows, cols, img_idx+4)\n",
    "            if img_idx == 0:\n",
    "                ax[ax_idx][3].set_title(\"Predicted\") # Set title for colum 3\n",
    "\n",
    "            img_to_model_2 = img[2*i+1].unsqueeze(dim=0) #.to(config.DEVICE)\n",
    "            # pred_out_2 = model(img_to_model_2)\n",
    "            # pred_out_2 = pred_out_2.permute(0, 2, 3, 1)\n",
    "            \n",
    "            pred_out_2 = onnx_predict(\n",
    "                img_to_model_2,\n",
    "                classify_session,\n",
    "                detect_session,\n",
    "                classification_thres=classification_thres\n",
    "            )\n",
    "                \n",
    "            pred_pic_2 = plot_predicted_img(\n",
    "                img[2*i+1], \n",
    "                pred_out_2, \n",
    "                score_thres=score_thres,\n",
    "                grid=False)\n",
    "\n",
    "            ax[ax_idx][3].imshow(pred_pic_2)\n",
    "            ax[ax_idx][3].set_axis_off()\n",
    "\n",
    "            img_idx += 4 # Move to next row\n",
    "            #print(f'Img idx after update: {img_idx}')\n",
    "            \n",
    "            if int(img_idx/2) == n_imgs:\n",
    "                #print(\"Break Inner Loop\")\n",
    "                break\n",
    "        \n",
    "        if int(img_idx/2) == n_imgs:\n",
    "            #print(\"Break Outer Loop: data loader\")\n",
    "            break\n",
    "    \n",
    "    plt.tight_layout(pad=0.5)\n",
    "\n",
    "    if save_name is not None:\n",
    "        plt.savefig(save_name + '.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5160f9-b242-4a12-8750-995efd29144b",
   "metadata": {},
   "source": [
    "# Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72b6c601-eace-4aaf-9581-f3cd53709c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMGS_PLOT = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255632de-5a0d-4e10-8441-6f63c60952e7",
   "metadata": {},
   "source": [
    "### Only Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed276b7-ca9b-42ad-82a3-f326b9c56714",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/onnx_eval/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/envs/onnx_eval/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/onnx_eval/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/opt/conda/envs/onnx_eval/lib/python3.10/shutil.py\", line 731, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/opt/conda/envs/onnx_eval/lib/python3.10/shutil.py\", line 729, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-u1iba46b'\n"
     ]
    }
   ],
   "source": [
    "plot_n_images_onnx(\n",
    "    loader = val_loader, \n",
    "    classification_model =None,\n",
    "    detection_model = './onnx_models/w8a8b8__bed_detector___aimet__fixed_point__qcdq__CPU.onnx', \n",
    "    classification_thres = 0, # No impact in this case\n",
    "    score_thres = 0.2,\n",
    "    n_imgs = N_IMGS_PLOT,\n",
    "    save_name = save_only_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740aca01-3c0f-40b4-9fca-f7dbadd16c8e",
   "metadata": {},
   "source": [
    "### Classify 1st + Detect 2nd: Classification Thres (50 %) = 0, Score Threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b9a583-f0be-47a9-91a3-c81c19b51594",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_images_onnx(\n",
    "    loader = val_loader, \n",
    "    classification_model ='./onnx_models/medium_fassd__conv341_big__epoch=93.onnx',\n",
    "    detection_model = './onnx_models/w8a8b8__bed_detector___aimet__fixed_point__qcdq__CPU.onnx', \n",
    "    classification_thres = 0,\n",
    "    score_thres = 0.2,\n",
    "    n_imgs = N_IMGS_PLOT,\n",
    "    save_name = save_classify_05_detect_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5da049-a898-45f1-8d35-1b43dcf0e341",
   "metadata": {},
   "source": [
    "### Classify 1st + Detect 2nd: Classification Thres (30 %) = -0.8472978604, Score Threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf333c4-d178-4322-bc88-e8ce4cc4663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_images_onnx(\n",
    "    loader = val_loader, \n",
    "    classification_model ='./onnx_models/medium_fassd__conv341_big__epoch=93.onnx',\n",
    "    detection_model = './onnx_models/w8a8b8__bed_detector___aimet__fixed_point__qcdq__CPU.onnx', \n",
    "    classification_thres = -0.8472978604,\n",
    "    score_thres = 0.2,\n",
    "    n_imgs = N_IMGS_PLOT,\n",
    "    save_name = save_classify_03_detect_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09660cfb-b5ea-48b1-b1fc-3bfaf59dfcbc",
   "metadata": {},
   "source": [
    "### Classify 1st + Detect 2nd: Score Threshold = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2f2276-956c-40af-ac8b-979c3857d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_n_images_onnx(\n",
    "#     loader = val_loader, \n",
    "#     classification_model ='./onnx_models/medium_fassd__conv341_big__epoch=93.onnx',\n",
    "#     detection_model = './onnx_models/w8a8b8__bed_detector___aimet__fixed_point__qcdq__CPU.onnx', \n",
    "#     score_thres = 0.15,\n",
    "#     n_imgs = N_IMGS_PLOT,\n",
    "#     save_name = save_classify_detect_015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31967195-750f-4547-a274-743311ab2a6a",
   "metadata": {},
   "source": [
    "### Classify 1st + Detect 2nd: Score Threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e826b6-3e64-44fb-a78e-e28b9ac304dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_n_images_onnx(\n",
    "#     loader = val_loader, \n",
    "#     classification_model ='./onnx_models/medium_fassd__conv341_big__epoch=93.onnx',\n",
    "#     detection_model = './onnx_models/w8a8b8__bed_detector___aimet__fixed_point__qcdq__CPU.onnx', \n",
    "#     score_thres = 0.1,\n",
    "#     n_imgs = N_IMGS_PLOT,\n",
    "#     save_name = save_classify_detect_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5335aac-2bc9-4c41-8bbe-3659b35ca04c",
   "metadata": {},
   "source": [
    "### Classify 1st + Detect 2nd: Score Threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67007a86-9450-42b4-bad3-ec3fba2e6038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_n_images_onnx(\n",
    "#     loader = val_loader, \n",
    "#     classification_model ='./onnx_models/medium_fassd__conv341_big__epoch=93.onnx',\n",
    "#     detection_model = './onnx_models/w8a8b8__bed_detector___aimet__fixed_point__qcdq__CPU.onnx', \n",
    "#     score_thres = 0.01,\n",
    "#     n_imgs = N_IMGS_PLOT,\n",
    "#     save_name = save_classify_detect_001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
