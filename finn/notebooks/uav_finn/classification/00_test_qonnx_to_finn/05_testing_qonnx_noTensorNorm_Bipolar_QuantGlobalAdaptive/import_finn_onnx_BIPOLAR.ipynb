{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719af929-1f81-4c5a-bfa4-1d45ba1d942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "from finn.util.test import get_test_model_trained\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "import numpy as np\n",
    "import qonnx.core.onnx_exec as oxe\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37ec5e-5577-49c5-bbae-00afe5f236bc",
   "metadata": {},
   "source": [
    "### Export to QONNX during training and test with DFire MINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be0a8a1-6e61-4704-ba78-a4ecb080410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_onnx = 'BED_classifier__best_mean_F1__BIPOLAR_Out____NoTensorNorm__QONNX.onnx'\n",
    "clean_onnx = '01_clean.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95089f8d-4875-4863-8d1b-9606f6191690",
   "metadata": {},
   "outputs": [],
   "source": [
    "qonnx_cleanup(ori_onnx, out_file=clean_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e34bbd8-9300-4ca3-9fca-5a5656946785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'BED_classifier__best_mean_F1__BIPOLAR_Out____NoTensorNorm__QONNX.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7be0ed1ae0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(ori_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1035b16-9a56-413c-b3f0-3ff9646c5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving '01_clean.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7ae020f370>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(clean_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8cfc3e-f0b1-4609-b410-9336bb4d73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_tidy_onnx =  '02_finn_tidy.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5997dd4-0a00-4cde-9311-47ae0441c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(clean_onnx)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(finn_tidy_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f698f2-2ba8-4ad1-9c50-07c71ed6bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving '02_finn_tidy.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7be0ed20e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_tidy_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486879b-97df-430d-bd7c-e8082339675a",
   "metadata": {},
   "source": [
    "# Compare Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06a1cc8-12f6-482e-a83e-ab133305b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = np.random.randint(low = 0, high = 255+1, size = (1, 3, 224, 224)) \n",
    "dummy_in = (dummy_in / 256.).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae2359d-3b1a-482d-abc0-b928543d1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ori_model = ModelWrapper(ori_onnx)\n",
    "clean_model = ModelWrapper(clean_onnx)\n",
    "finn_model = ModelWrapper(finn_tidy_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596967b-69c4-4466-b7c5-ad938fb9cd20",
   "metadata": {},
   "source": [
    "### Cleaned QONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22786da-5144-4596-85ea-014d61ad8f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = {\"global_in\": dummy_in}\n",
    "output_dict = oxe.execute_onnx(clean_model, input_dict)\n",
    "produced_clean_qonnx = output_dict[list(output_dict.keys())[0]]\n",
    "produced_clean_qonnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f795ad-972f-4e27-8ca4-1965b3a4d998",
   "metadata": {},
   "source": [
    "### FINN QONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78b7c553-1f25-4c64-b5fd-a5a1d30c5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/util/basic.py:296: UserWarning: The values of tensor MultiThreshold_0_out0 can't be represented with the set datatype annotation (INT8), they will be rounded to match the datatype annotation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = {\"global_in\": dummy_in}\n",
    "output_dict = oxe.execute_onnx(finn_model, input_dict)\n",
    "produced_finn_qonnx = output_dict[list(output_dict.keys())[0]]\n",
    "produced_finn_qonnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31643563-b279-4aff-841f-1a8a8922dcb3",
   "metadata": {},
   "source": [
    "# Compare with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "678df304-6e6f-48aa-b5f1-a6eb4f47aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = './WEB10495.jpg' # Smoke and Fire\n",
    "#img_file = './WEB10980.jpg' # Only Smoke\n",
    "#img_file = './WEB10031.jpg' # Empty\n",
    "img = cv2.imread(img_file)\n",
    "img = cv2.imread(img_file)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "img = cv2.resize(img, (224,224), interpolation = cv2.INTER_LINEAR) \n",
    "img = img / 256.\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = np.transpose(img, (0, 3, 1, 2))\n",
    "img = img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b266fad5-762a-4a13-a5a9-4267a61b1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1, 3, 224, 224)\n",
      "Data type: float32\n"
     ]
    }
   ],
   "source": [
    "print(f'Image shape: {img.shape}')\n",
    "print(f'Data type: {img.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e918859-309c-44b6-84de-80c7c30776ab",
   "metadata": {},
   "source": [
    "### Cleaned QONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb367365-3176-40bb-ad8a-08caa3c39dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = {\"global_in\": img}\n",
    "output_dict = oxe.execute_onnx(clean_model, input_dict)\n",
    "produced_clean_qonnx = output_dict[list(output_dict.keys())[0]]\n",
    "produced_clean_qonnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db5dc1-5c7e-4855-8b7b-dcebdc524ee0",
   "metadata": {},
   "source": [
    "### FINN QONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e47a245-b34d-453a-8c32-6ab4c0ca1ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict = {\"global_in\": img}\n",
    "output_dict = oxe.execute_onnx(finn_model, input_dict)\n",
    "produced_finn_qonnx = output_dict[list(output_dict.keys())[0]]\n",
    "produced_finn_qonnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1055e1e-e282-47d5-a9a1-9c11cb671af0",
   "metadata": {},
   "source": [
    "# Add Preprocessing Node: /255. -> ToTensor, FINN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fbc7b55-39d1-424f-a9fb-85922cc06015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8ac592-7345-4a36-881f-21f487449c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/transformation/merge_onnx_models.py:70: UserWarning: [MergeONNXModels] opsets for models to merge differ: 14 vs 17, output model will use opset 17\n",
      "  warnings.warn(\n",
      "/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper(finn_tidy_onnx)\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"./preproc_BED_input.onnx\"\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# join preprocessing and core model\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e13e5-6a77-4e4b-b9e4-941982814787",
   "metadata": {},
   "source": [
    "### Tidy up again and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bd4a4f4-a5bb-418b-9c7e-5d64703f1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "finn_tidy_preprocess = '03_finn_prepro.onnx'\n",
    "model.save(finn_tidy_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "483ebf73-1d7a-4e54-a4a1-c324c952ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving '03_finn_prepro.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7be0ed0d90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_tidy_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc753d-aa69-4b18-baad-ef91fb7309b7",
   "metadata": {},
   "source": [
    "# Streamline the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f8c0052-79dd-4b02-affe-4f817b94e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Streamline(Transformation):\n",
      "    \"\"\"Apply the streamlining transform, see arXiv:1709.04060.\"\"\"\n",
      "\n",
      "    def apply(self, model):\n",
      "        streamline_transformations = [\n",
      "            ConvertSubToAdd(),\n",
      "            ConvertDivToMul(),\n",
      "            BatchNormToAffine(),\n",
      "            ConvertSignToThres(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            MoveScalarLinearPastInvariants(),\n",
      "            AbsorbSignBiasIntoMultiThreshold(),\n",
      "            MoveAddPastMul(),\n",
      "            MoveScalarAddPastMatMul(),\n",
      "            MoveAddPastConv(),\n",
      "            MoveScalarMulPastMatMul(),\n",
      "            MoveScalarMulPastConv(),\n",
      "            MoveAddPastMul(),\n",
      "            CollapseRepeatedAdd(),\n",
      "            CollapseRepeatedMul(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            AbsorbAddIntoMultiThreshold(),\n",
      "            FactorOutMulSignMagnitude(),\n",
      "            AbsorbMulIntoMultiThreshold(),\n",
      "            Absorb1BitMulIntoMatMul(),\n",
      "            Absorb1BitMulIntoConv(),\n",
      "            RoundAndClipThresholds(),\n",
      "        ]\n",
      "        for trn in streamline_transformations:\n",
      "            model = model.transform(trn)\n",
      "            model = model.transform(RemoveIdentityOps())\n",
      "            model = model.transform(GiveUniqueNodeNames())\n",
      "            model = model.transform(GiveReadableTensorNames())\n",
      "            model = model.transform(InferDataTypes())\n",
      "        return (model, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from finn.util.visualization import showSrc\n",
    "from finn.transformation.streamline import Streamline\n",
    "showSrc(Streamline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "271b2898-00f3-4e57-87ba-dc374446ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "import finn.transformation.streamline.reorder as reorder\n",
    "from qonnx.transformation.change_datalayout import ChangeDataLayoutQuantAvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd267e01-e10f-4dd3-bac5-168830c93cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(finn_tidy_preprocess)\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(ChangeDataLayoutQuantAvgPool2d()) # Wortel\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c14939ea-f1d4-47d9-b445-b636b6defead",
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_streamline = './04_finn_streamline.onnx'\n",
    "model.save(finn_streamline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f55cf712-a762-46ac-9869-1cf2f664f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './04_finn_streamline.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7ad02307c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_streamline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea548f00-20b8-4983-9c41-9ecadb383c1b",
   "metadata": {},
   "source": [
    "# Partitioning, Conversion to HW Layers and Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3eff73b7-e2f8-4ce6-b9b7-4d58e7663935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import pynq_part_map\n",
    "# change this if you have a different PYNQ board, see list above\n",
    "pynq_board = \"Pynq-Z1\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a1297fa-1ca8-40c7-9bb2-95cf219c9da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xc7z020clg400-1\n"
     ]
    }
   ],
   "source": [
    "print(fpga_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "441d42c2-1292-458c-b5fa-569089be0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a74e1-d4ed-42c9-bdd4-49c54955a471",
   "metadata": {},
   "source": [
    "### HW Layers\n",
    "\n",
    "https://github.com/Xilinx/finn/blob/main/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py\n",
    "\n",
    "Added InferGlobalAccPoolLayer to transform GlobalAvgPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32b62375-19f5-4328-b9d6-15bc983fe663",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(finn_streamline)\n",
    "\n",
    "# MVAUs\n",
    "#model = model.transform(to_hw.InferBinaryMatrixVectorActivation()) # -> Wort: no binary mtx mult in my model\n",
    "#model = model.transform(to_hw.InferVectorVectorActivation()) # -> Wort: esto es para conv depthwise\n",
    "model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model = model.transform(to_hw.InferConvInpGen())\n",
    "model = model.transform(to_hw.InferStreamingMaxPool())\n",
    "model = model.transform(to_hw.InferPool())\n",
    "\n",
    "# get rid of Reshape(-1, 1) operation between hw nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7977d53-ea29-471e-a32c-d11b5c32a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_hw_layers = './05_finn_hw_layers.onnx'\n",
    "model.save(finn_hw_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcf93a21-56cf-4546-a52d-ca7653ef8c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './05_finn_hw_layers.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7ad0414280>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_hw_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72002a73-7f65-4e1b-a331-4f7bb3c293fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "cycle-free graph violated: partition depends on itself",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parent_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCreateDataflowPartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m parent_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./BED_classifier__best_mean_F1_finn_parent__perChannel_fxPoint__QONNX.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m sdp_node \u001b[38;5;241m=\u001b[39m parent_model\u001b[38;5;241m.\u001b[39mget_nodes_by_op_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamingDataflowPartition\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/gmoreno/uav/finn/src/finn/transformation/fpgadataflow/create_dataflow_partition.py:80\u001b[0m, in \u001b[0;36mCreateDataflowPartition.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# first, use the generic partitioning functionality to split up the graph\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m parent_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPartitionFromLambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massign_partition_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_model_dir\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# change node types to StreamingDataflowPartition\u001b[39;00m\n\u001b[1;32m     86\u001b[0m p_nodes \u001b[38;5;241m=\u001b[39m parent_model\u001b[38;5;241m.\u001b[39mget_nodes_by_op_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenericPartition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/transformation/create_generic_partitions.py:119\u001b[0m, in \u001b[0;36mPartitionFromLambda.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m to_check:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 119\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitioning(node) \u001b[38;5;241m!=\u001b[39m partition_id\n\u001b[1;32m    120\u001b[0m         ), \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mcycle-free graph violated: partition depends on itself\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;66;03m# print(node)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         predecessors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfind_direct_predecessors(node)\n",
      "\u001b[0;31mAssertionError\u001b[0m: cycle-free graph violated: partition depends on itself"
     ]
    }
   ],
   "source": [
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save('./BED_classifier__best_mean_F1_finn_parent__perChannel_fxPoint__QONNX.onnx')\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "# and specialize the layers to HLS variants\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model = dataflow_model.transform(SpecializeLayers(fpga_part))\n",
    "dataflow_model.save('./BED_classifier__best_mean_F1_finn_dataflow__perChannel_fxPoint__QONNX.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e1454-bd5a-4b78-be69-431805de6943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
