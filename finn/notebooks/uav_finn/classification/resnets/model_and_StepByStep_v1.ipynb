{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463c1477-b429-4abe-b16e-f51487ed935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.nn import QuantIdentity\n",
    "from brevitas.nn import QuantConv2d\n",
    "from brevitas.nn import QuantReLU\n",
    "from brevitas.nn import TruncAvgPool2d\n",
    "from brevitas.nn import QuantLinear\n",
    "\n",
    "from brevitas.quant import TruncTo8bit\n",
    "\n",
    "\n",
    "from brevitas.quant import Int8ActPerTensorFloat # For Quant ADD node\n",
    "from common_imagenet import CommonIntWeightPerTensorQuant\n",
    "from common_imagenet import CommonIntWeightPerChannelQuant\n",
    "from common_imagenet import CommonUintActQuant\n",
    "from common_imagenet import CommonIntActQuant # For initial Q1.7 Identity Layer\n",
    "from tensor_norm import TensorNorm\n",
    "\n",
    "from brevitas.export import export_qonnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c9011-85ab-4877-b5c8-c6c45e8f53bf",
   "metadata": {},
   "source": [
    "# Custom Quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487e789f-3ca2-454c-8f26-a1102cd55645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWeightsQuant_PerTensor(CommonIntWeightPerTensorQuant):\n",
    "    restrict_scaling_type = RestrictValueType.POWER_OF_TWO\n",
    "\n",
    "class MyWeightsQuant_PerChannel(CommonIntWeightPerChannelQuant):\n",
    "    restrict_scaling_type = RestrictValueType.POWER_OF_TWO\n",
    "\n",
    "class MyReLUQuant(CommonUintActQuant):\n",
    "    restrict_scaling_type = RestrictValueType.POWER_OF_TWO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fce3e-24f8-4e2b-9a92-3ba716faea5f",
   "metadata": {},
   "source": [
    "# Tiny Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f7262e-c8c8-4b2f-abf8-f7ea70e8e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TINY_RESNET(Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_classes = 2, \n",
    "                 weight_bit_width = 4,\n",
    "                 act_bit_width = 4, \n",
    "                 in_bit_width = 8, \n",
    "                 in_channels = 3):\n",
    "        super(TINY_RESNET, self).__init__()\n",
    "        \n",
    "        self.conv_features = ModuleList()\n",
    "        self.conv_branch = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "\n",
    "        # Input 230x230x3\n",
    "        self.conv_features.append(QuantIdentity( # for Q1.7 input format -> sign.7bits\n",
    "            act_quant = CommonIntActQuant,\n",
    "            bit_width = in_bit_width,\n",
    "            min_val = -1.0,\n",
    "            max_val = 1.0 - 2.0 ** (-7),\n",
    "            narrow_range = False, \n",
    "            restrict_scaling_type = RestrictValueType.POWER_OF_TWO))\n",
    "\n",
    "        # CNNBlock 224x224\n",
    "            # conv1\n",
    "        self.conv_features.append(\n",
    "            QuantConv2d(\n",
    "                kernel_size=3, stride=1, padding=1,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=12,\n",
    "                bias=False,\n",
    "                weight_quant=MyWeightsQuant_PerTensor,\n",
    "                weight_bit_width=weight_bit_width))\n",
    "        self.conv_features.append(BatchNorm2d(12))\n",
    "        self.conv_features.append(\n",
    "            QuantReLU(\n",
    "                act_quant=MyReLUQuant,\n",
    "                bit_width=act_bit_width, \n",
    "                return_quant_tensor=True))\n",
    "        \n",
    "        self.conv_features.append(MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # CNNBlock 112x112\n",
    "            # conv2\n",
    "        self.conv_branch.append(\n",
    "            QuantConv2d(\n",
    "                kernel_size=3, stride=1, padding=1,\n",
    "                in_channels=12,\n",
    "                out_channels=12,\n",
    "                bias=False,\n",
    "                weight_quant=MyWeightsQuant_PerTensor,\n",
    "                weight_bit_width=weight_bit_width))\n",
    "        self.conv_branch.append(BatchNorm2d(12))\n",
    "        self.conv_branch.append(\n",
    "            QuantReLU(\n",
    "                act_quant=MyReLUQuant,\n",
    "                bit_width=act_bit_width, \n",
    "                return_quant_tensor=True))\n",
    "\n",
    "        # Convs Branch\n",
    "        self.quant_MP_0_0 = QuantIdentity( \n",
    "            act_quant = Int8ActPerTensorFloat,\n",
    "            bit_width = 4, \n",
    "            restrict_scaling_type = RestrictValueType.POWER_OF_TWO)\n",
    "        self.quant_MP_0_1 = QuantIdentity( \n",
    "            act_quant = self.quant_MP_0_0.act_quant,\n",
    "            bit_width = 4, \n",
    "            restrict_scaling_type = RestrictValueType.POWER_OF_TWO,\n",
    "            return_quant_tensor=True)\n",
    "        # Direct Branch\n",
    "        self.quant_MP_1 = QuantIdentity( \n",
    "            act_quant = self.quant_MP_0_0.act_quant,\n",
    "            bit_width = 4, \n",
    "            restrict_scaling_type = RestrictValueType.POWER_OF_TWO,\n",
    "            return_quant_tensor=True)\n",
    "\n",
    "        self.quant_add = QuantIdentity( \n",
    "            act_quant = Int8ActPerTensorFloat,\n",
    "            bit_width = 4, \n",
    "            restrict_scaling_type = RestrictValueType.POWER_OF_TWO,\n",
    "            return_quant_tensor=True)\n",
    "        \n",
    "        self.avg_pool = TruncAvgPool2d(\n",
    "                kernel_size=112,  \n",
    "                trunc_quant=TruncTo8bit,\n",
    "                float_to_int_impl_type='FLOOR')\n",
    "\n",
    "        # Linear 1\n",
    "        self.linear_features.append(\n",
    "            QuantLinear(\n",
    "                in_features=12,\n",
    "                out_features=8,\n",
    "                bias=False,\n",
    "                weight_quant=MyWeightsQuant_PerTensor,\n",
    "                weight_bit_width=weight_bit_width))\n",
    "        self.linear_features.append(BatchNorm1d(8))\n",
    "        self.linear_features.append(\n",
    "            QuantReLU(\n",
    "                act_quant=MyReLUQuant,\n",
    "                bit_width=act_bit_width, \n",
    "                return_quant_tensor=False))\n",
    "\n",
    "        # Linear 2\n",
    "        self.linear_features.append(\n",
    "            QuantLinear(\n",
    "                in_features=8,\n",
    "                out_features=2,\n",
    "                bias=False,\n",
    "                weight_quant=MyWeightsQuant_PerTensor,\n",
    "                weight_bit_width=weight_bit_width))\n",
    "        self.linear_features.append(TensorNorm())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "\n",
    "        x_res = self.quant_MP_1(x)\n",
    "        x_conv = self.quant_MP_0_0(x)\n",
    "        x_conv = self.conv_branch[0](x_conv)\n",
    "        x_conv = self.conv_branch[1](x_conv)\n",
    "        x_conv = self.conv_branch[2](x_conv)\n",
    "        x_conv = self.quant_MP_0_1(x_conv)\n",
    "        \n",
    "        x = x_conv + x_res\n",
    "        x = self.quant_add(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f861ce-1362-4bec-b658-818c517a24c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreno/uav/finn/deps/brevitas/src/brevitas/nn/mixin/base.py:77: UserWarning: Keyword arguments are being passed but they not being used.\n",
      "  warn('Keyword arguments are being passed but they not being used.')\n"
     ]
    }
   ],
   "source": [
    "model_qnn = TINY_RESNET().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c34946d-14b2-4802-9dec-a6fd402f3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 3, 224, 224)\n",
    "# print(summary(model_qnn, input_size=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa1f10b-4eba-4fd1-978b-6448998826ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = './step_by_step_tiny'\n",
    "model_qnn_filename = models_folder + '/TINY_Resnet__QONNX.onnx' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a098b0-f1db-45dc-89c9-57ceb9f84648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qnn.eval();\n",
    "export_qonnx(model_qnn, torch.randn(input_shape), model_qnn_filename);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fed44b-915d-4f13-bf47-f09b669a065d",
   "metadata": {},
   "source": [
    "# FINN Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda68f6-24c1-4eb3-8796-62c91f570e47",
   "metadata": {},
   "source": [
    "## Load Model and View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bf723b71-f555-490c-b185-776f73396c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7dff265e-bed5-4efd-92a6-1ee06471f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './step_by_step_tiny/TINY_Resnet__QONNX.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7f4e15960>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_qnn_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52bb090-4801-4cd6-91f4-e86048dfcb11",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2150e426-3ba5-4aab-b680-78cd716e2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qonnx_clean_filename = models_folder + '/01_clean.onnx'\n",
    "qonnx_cleanup(model_qnn_filename, out_file=qonnx_clean_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "42d9a172-78a4-404f-be19-334b6747409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './step_by_step_tiny/01_clean.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7f07a81c0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(qonnx_clean_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71cacf-447e-4a5e-a628-1e4b5188b6a8",
   "metadata": {},
   "source": [
    "## Convert to FINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6c2d3af5-b4df-47ba-af6b-2391ba0950c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "89512ff2-3de9-435d-9ac0-459a5b4f30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "622516fa-c609-4dc0-86a4-19aafedaaa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(qonnx_clean_filename)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b36e1b0a-eec1-476d-ac12-6919a911d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_tidy = models_folder + '/02_finn_tidy.onnx'\n",
    "model.save(finn_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eb2c21cb-492c-4cee-8e1b-ec8784405cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './step_by_step_tiny/02_finn_tidy.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7f4ca7700>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9fec5-a30c-4d6f-8e50-4bfb3a674a3f",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6b39deb7-0653-4b54-bb09-ac774320547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6f39b02d-f548-4fb8-baa2-9bd0add11fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper(finn_tidy)\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = models_folder + \"/prepro_node.onnx\"\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# join preprocessing and core model\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994573e-3b15-4f90-9e4e-4d82b9288ad0",
   "metadata": {},
   "source": [
    "### Tidy again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "87c667fd-97b6-4bf5-bd82-ae0f97353bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3c8d211b-0a24-4381-b2fe-cc9e92eac928",
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_prepro = models_folder + '/03_finn_prepro.onnx'\n",
    "model.save(finn_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "11e7ee6e-aa4b-4e6b-9b32-4ad8d1a45911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './step_by_step_tiny/03_finn_prepro.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff8280cc130>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426532b9-8327-4539-a3da-83dbb47a5899",
   "metadata": {},
   "source": [
    "## Streamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "deabcd45-ecb5-4e18-b104-862f4c6a8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "\n",
    "from qonnx.transformation.change_datalayout import ChangeDataLayoutQuantAvgPool2d\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC\n",
    "from finn.transformation.streamline.reorder import MoveLinearPastEltwiseAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d96d965e-7920-4183-b3ac-7e6375911892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreno/uav/finn/src/finn/transformation/streamline/absorb.py:66: UserWarning: Threshold or add bias not constant, skipping\n",
      "  warnings.warn(\"Threshold or add bias not constant, skipping\")\n"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper(finn_prepro)\n",
    "model = model.transform(MoveLinearPastEltwiseAdd())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(ChangeDataLayoutQuantAvgPool2d())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "864f1a68-a3be-4aa7-8d18-ae6da439c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_streamline = models_folder + '/04_finn_streamline.onnx'\n",
    "model.save(finn_streamline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fb88790d-6012-42bc-93e3-7ec9a5aa4c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './step_by_step_tiny/04_finn_streamline.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff82013f670>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_streamline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f73da-659c-4885-8f3a-05c45d38d15c",
   "metadata": {},
   "source": [
    "# To HW Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1a8179a6-5bce-4bcc-8ea7-5bdfdbe73b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "\n",
    "from qonnx.custom_op.registry import getCustomOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8b83ce29-5b8b-4231-a5cb-8ae4e17e32c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "MultiThreshold_5: Signed output requires actval < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(to_hw\u001b[38;5;241m.\u001b[39mInferQuantizedMatrixVectorActivation())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# input quantization (if any) to standalone thresholding\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_hw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferThresholdingLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(to_hw\u001b[38;5;241m.\u001b[39mInferPool())\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(to_hw\u001b[38;5;241m.\u001b[39mInferStreamingMaxPool())\n",
      "File \u001b[0;32m/home/gmoreno/uav/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/gmoreno/uav/finn/src/finn/transformation/fpgadataflow/convert_to_hw_layers.py:248\u001b[0m, in \u001b[0;36mInferThresholdingLayer.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# a signed activation should always have a negative bias,\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# but BIPOLAR uses the -1 as 0 encoding so the assert does not apply\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m odt \u001b[38;5;241m!=\u001b[39m DataType[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBIPOLAR\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m odt\u001b[38;5;241m.\u001b[39msigned()) \u001b[38;5;129;01mor\u001b[39;00m (actval \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m), (\n\u001b[1;32m    249\u001b[0m         node\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: Signed output requires actval < 0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m     )\n\u001b[1;32m    252\u001b[0m new_node \u001b[38;5;241m=\u001b[39m helper\u001b[38;5;241m.\u001b[39mmake_node(\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThresholding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    254\u001b[0m     [thl_input, thl_threshold],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThresholding_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    269\u001b[0m graph\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39minsert(insert_point, new_node)\n",
      "\u001b[0;31mAssertionError\u001b[0m: MultiThreshold_5: Signed output requires actval < 0"
     ]
    }
   ],
   "source": [
    "model = ModelWrapper(finn_streamline)\n",
    "\n",
    "model = model.transform(to_hw.InferAddStreamsLayer())\n",
    "model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model = model.transform(to_hw.InferPool())\n",
    "model = model.transform(to_hw.InferStreamingMaxPool())\n",
    "model = model.transform(to_hw.InferConvInpGen())\n",
    "\n",
    "# get rid of Reshape(-1, 1) operation between hw nodes \n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "\n",
    "model = model.transform(Streamline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a82b32cb-dd15-44db-81d2-645b4194d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_hw_layers = models_folder + '/05_fin_hw_layers.onnx'\n",
    "model.save(finn_hw_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c352640a-5628-4b96-ba85-2997aa8fe2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving './step_by_step_tiny/05_fin_hw_layers.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7f7f2a440>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_hw_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa8e19-c37b-44ea-a0b8-6844587f3363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
